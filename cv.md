Adam L. Berenzweig
===============

----

<table width=100%>
<tr>
<td>adam.b@gmail.com</td>
<td>[https://www.linkedin.com/in/madadam/](https://www.linkedin.com/in/madadam/)</td>
</tr>
</table>

----

## Interests, goals, and expertise.

I've spent most of my career building products around machine learning.  I enjoy taking the
latest research breakthroughs and figuring out how to apply them in useful, high-impact projects.
I have broad knowledge of systems architecture, the product lifecycle, and machine learning.

Research and engineering expertise includes:

* Machine learning, information retrieval, recommender systems.
* Machine perception (speech and vision, auditory scene analysis).
* Neuromotor interfaces, HCI, UI/UX and interaction design.
* NLP, language models, linguistics.
* Knowledge graphs and reconciliation.
* Engineering large-scale distributed systems, RPC services and data workflow systems.
* Engineering and operations best practices, continuous deployment and testing.
* Engineering management.


## CTRL-labs and Meta

In June 2016, I joined CTRL-labs, a neurotech startup
building a practical neural interface, worn on the arm, that decodes the signals flowing
from your brain to your hand. The goal was to be the primary input technology
for the next personal computing platform -- when your computer is worn on your body,
such as eyeglasses.

As Director of R&D, my role was to ask "What is this thing good for?" I launched and led projects,
steered the technical direction of the science
and human-interactions teams, and contributed directly to core demos used for fund-raising and
eventually our
[acquisition by Facebook/Meta](https://www.cnbc.com/2019/09/23/facebook-announces-acquisition-of-brain-computing-start-up-ctrl-labs.html).

At Meta, my focus was solving the text input problem, for the times you don't want to talk out loud to your computer. I started and led
two big efforts, first around two-hand surface typing and then handwriting (appropriate for a one-handed product scenario).
- The surface-typing project was reported as the emg2qwerty dataset, open-sourced in a NeurIPS 2024 data track paper.
- The handwriting decoder was reported as one of the tasks in our group's foundational EMG paper, accepted in Nature (available in preprint on bioarxiv) and visible in the demo shown at ICASSP 2024 (below).

In early 2026, [handwriting with the Neural Band](https://www.engadget.com/wearables/handwriting-is-my-new-favorite-way-to-text-with-the-meta-ray-ban-display-glasses-213744708.html) launched for the Meta Ray-Ban Display.

Talks, press, and podcasts:

* [Rethinking HCI with Neural Interfaces](https://www.infoq.com/presentations/hci-ctrllabsco/),
  QCON 2018
* [Practical AI](https://changelog.com/practicalai/41), The
  Changelog's AI podcast.
* [Brain-Machine Interface Isn't Sci-fi Anymore](https://www.wired.com/story/brain-machine-interface-isnt-sci-fi-anymore/), Steven Levy, Wired, September 2017.
* [Facebook buys startup building neural monitoring armband](https://techcrunch.com/2019/09/23/facebook-buys-startup-building-neural-monitoring-armband/),
  TechCrunch, September 2019.
* [Handwriting is my new favorite way to text with the Meta Ray-Ban Display glasses](https://www.engadget.com/wearables/handwriting-is-my-new-favorite-way-to-text-with-the-meta-ray-ban-display-glasses-213744708.html), Engadget, Jan 2026.

## Clarifai

In early 2014 I helped start [Clarifai](http://www.clarifai.com), a machine learning and image recognition company, with Matt Zeiler.  I served as CTO from founding through 2016.

Accomplishments:

* Launched an image and video recognition service backed by deep convolutional neural networks
  running on GPUs.
* Launched a search and indexing service with visual search, auto-tagging, visual clustering, and
  combined visual/metadata query capabilities.
* Raised a series A, grew from zero to 30 employees and significant revenue.
* Launched [Forevery](https://itunes.apple.com/app/apple-store/id1005812175?mt=8),
  a consumer photo organization mobile app powered by Clarifai's API.


Some talks I've given about Clarifai and current trends in computing driven by machine learning:

* [Databite 38](http://www.datasociety.net/events/databite-no-38-adam-berenzweig/):
  Deep learning, machine perception, and the future of memory.
* [Stanford VLAB panel on deep learning](https://youtu.be/czLI3oLDe8M?t=1349).

## Google

I was a software engineer at Google from 2003-2014.
A brief overview of projects and accomplishments:

2010-2012
:   **Music Recommendation for Google Play Music**

    I was tech lead of the music recommendation team during the initial beta launch, and architected
    the recommendation service.  I designed and helped build features and components such as:

    * Instant Mix (content-based playlist generation).  Covered here on the 
    [Google Research Blog](http://googleresearch.blogspot.com/2011/06/instant-mix-for-music-beta-by-google.html).
    * Personalized artist recommendations, using Samy Bengio and Jason Weston's [wsabie embeddings](http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf) for audio.
    * Improved recommendations in the presence of a query, using [Latent Collaborative Retrieval](http://research.google.com/pubs/pub40569.html) (with Jason Weston and Ron Weiss).
    * Numerous engineering projects, including the recommendation signals data pipeline, the metadata service, and
    music metadata reconciliation for the Knowledge Graph.

2007-2008
:   **Google News**

    I led a team to build the first large-scale application of NLP to Google News, producing
    named entity annotations and using them in various ways:

    * to improve search result ranking for certain news queries.
    * to improve clustering of news articles into stories that evolve over time
    * to extract authors from the byline


2009-2010
:   **Realtime Search**

    [Realtime Search](https://en.wikipedia.org/wiki/Google_Real-Time_Search) was
    the first project to provide search results (mostly from Twitter) that streamed in
    real-time on the Google search results page.  I worked on the search infrastructure to support
    real-time streaming.  The project won an OC (Organizing Committee) award.


2012-2013
:   **Goggles / Visual Search / Google Glass**

    I worked on Goggles, the first widely-used image recognition
    mobile app for consumers, during the transition from standalone
    app into Google Photos.  I was also involved with some experiments
    to use recognition and computer vision in Google Glass,
    specifically for a timelapse life-logging system. Goggles was eventually
    as Google Lens.

2013
:   **Colaboratory**

    I helped steer a project to build a new document-based computing tool that unifies code with text
    and interactive visualization.  I developed the product vision and hacked on early
    proof-of-concept demos, such as:  an in-browser python kernel, two-way binding between data cells
    in the browser and in backend execution kernels, and interactive visualizations with editable
    javascript.

    The project was eventually launched as [Google Colab](https://colab.research.google.com/).

2003-2013
:   **Non-project accomplishments**

    * Started and organized the CSA (community-supported agriculture) program in NYC, a local farm
      share program. Grew to 80+ members in 3 years.
    * Taught an internal training class (Life of a Query)
    * Career mentoring


## Earlier Professional Experience

2001, 2002
:   *NEC Research Institute*, Princeton NJ.

    I interned at NEC for two summers of my Ph.D., during the heydey when Vladimir Vapnik, Yann
    LeCunn, Steve Lawrence, and others there were doing pioneering work in machine learning and
    information retrieval.  I worked with Brian Whitman on music information retrieval, 
    a precursor to his company The Echo Nest (now part of Spotify) and my work for the Google Play Music service.

1997-2000
:   *Lucent Technologies*, Whippany NJ.

    Security Standards Architect, Wireless Networks.  
    I designed security architectures for wireless phone networks, and represented Lucent in industry
    standards bodies.

1999-2000
:   *Learning In Progress, Inc.*, New York, NY.

    I co-founded an ed-tech company (we called it "e-learning" back then) with the mission of building better
    tools and games to teach computer programming and IT skills.  We were about 10 years too soon.

Summers 1995, 1996
:   *Hotjobs*, New York, NY

    After I built a website as an intern for a small technical recruiting firm in NY,
    the CEO asked me if I could put some of their
    job listings from the database up on the website.  The resulting site evolved into Hotjobs, one of
    the darlings of the first dot-com boom.


## Education

2006
:   Ph.D., Electrical Engineering, Columbia University

    * Dissertation: "Personalized music similarity metrics"
    * Advisor: Daniel P.W. Ellis

2002
:   M.S., Electrical Engineering, Columbia University

1997
:   B.S., Computer Science, Yale University

## Publications

See [Google Scholar](https://scholar.google.com/citations?user=kwylb3sAAAAJ).


## Non-professional

I engineered and programmed the software for
[Reflecting The Stars](http://thewindmillfactory.com/?portfolio=reflecting-the-stars),
a public art installation on the Hudson River, for
Jon Morris of [The Windmill Factory](http://www.thewindmillfactory.com/reflecting_the_stars.html).
Covered in [Wired](http://www.wired.com/underwire/2011/09/reflecting-the-stars) and
[The New York Times](http://video.nytimes.com/video/2011/10/06/arts/100000001096048/artsbeat-october-6-2011.html).

In a past life, I played and wrote music.  I like to climb and surf, learn new languages, travel,
and eat good food.
